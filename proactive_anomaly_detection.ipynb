{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7e626ca-8c4d-4c92-933a-2b4efc8ebbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3db09d25-a593-47a3-8a25-a88cfcd1afe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu121\n",
      "True\n",
      "NVIDIA GeForce RTX 2070\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9726e600-be00-41a4-83e1-fd0d2f26cbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['M-6.npy', 'M-1.npy', 'M-2.npy', 'S-2.npy', 'P-10.npy', 'T-4.npy', 'T-5.npy', 'F-7.npy', 'M-3.npy', 'M-4.npy', 'M-5.npy', 'P-15.npy', 'C-1.npy', 'C-2.npy', 'T-12.npy', 'T-13.npy', 'F-4.npy', 'F-5.npy', 'D-14.npy', 'T-9.npy', 'P-14.npy', 'T-8.npy', 'P-11.npy', 'D-15.npy', 'D-16.npy', 'M-7.npy', 'F-8.npy']\n"
     ]
    }
   ],
   "source": [
    "# csv_path = '/content/drive/MyDrive/rupa/archive/labeled_anomalies.csv'\n",
    "csv_path = r'C:\\Users\\skhandaker\\OneDrive - Oklahoma City University\\Documents\\Anomaly Detection Paper\\rupa\\archive\\labeled_anomalies.csv'\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "m_files = [f\"{chan_id}.npy\" for chan_id in df.loc[df['spacecraft']=='MSL', 'chan_id']]\n",
    "\n",
    "print(m_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "346cd914-a601-4016-ad15-80ce1240190a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated shape (rows): (73729, 55)\n"
     ]
    }
   ],
   "source": [
    "# test_dir = '/content/drive/MyDrive/rupa/archive/data/data/test'\n",
    "test_dir = r'C:\\Users\\skhandaker\\OneDrive - Oklahoma City University\\Documents\\Anomaly Detection Paper\\rupa\\archive\\data\\data\\test'\n",
    "\n",
    "# m_files = sorted(f for f in os.listdir(test_dir) if f.endswith('.npy') and f.startswith('M'))\n",
    "\n",
    "data_list = [np.load(os.path.join(test_dir, fname)) for fname in m_files]\n",
    "\n",
    "X = np.concatenate(data_list, axis=0)\n",
    "\n",
    "print(\"Concatenated shape (rows):\", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea34da66-4a1c-49c3-9a15-8ef4a963d225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-1.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-1.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [-1.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.86956522  0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-1.          0.          0.         ...  0.          0.\n",
      "   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9f978eb-59bc-48c6-b0f6-7abf6d74ba3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique embeddings: 25478\n",
      "Total time steps: 73729\n"
     ]
    }
   ],
   "source": [
    "all_rows = []\n",
    "\n",
    "for f in m_files:\n",
    "    data = np.load(os.path.join(test_dir, f))\n",
    "    all_rows.append(data)\n",
    "\n",
    "combined = np.concatenate(all_rows, axis=0).astype(np.float32)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(combined, dtype=np.float32)\n",
    "unique_embeddings = df.drop_duplicates()\n",
    "\n",
    "print(\"Unique embeddings:\", len(unique_embeddings))\n",
    "print(\"Total time steps:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b798862b-21d9-44b2-ad62-f0b1be5dcb40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             0    1    2    3    4    5    6    7    8    9   ...   45   46  \\\n",
      "0     -1.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
      "1     -1.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
      "2     -1.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
      "3     -1.000000  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
      "4     -1.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
      "...         ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "73324  1.043478  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
      "73425  0.260870  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
      "73481 -0.956522  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
      "73564 -0.913043  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
      "73616 -0.913043  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
      "\n",
      "        47   48   49   50   51   52   53   54  \n",
      "0      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "...    ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "73324  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "73425  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "73481  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "73564  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "73616  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[25478 rows x 55 columns]\n"
     ]
    }
   ],
   "source": [
    "print(unique_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12f7e02f-4c7c-410d-8de9-81fc155e3c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             0    1    2    3    4    5    6    7    8    9   ...   45   46  \\\n",
      "0     -1.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
      "1     -1.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
      "2     -1.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
      "3     -1.000000  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
      "4     -1.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
      "...         ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "73724 -1.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
      "73725 -0.956522  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
      "73726 -1.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
      "73727 -0.869565  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
      "73728 -1.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
      "\n",
      "        47   48   49   50   51   52   53   54  \n",
      "0      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "...    ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "73724  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "73725  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "73726  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "73727  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "73728  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[73729 rows x 55 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26856e60-a052-44a2-aa14-dff5f2ceff09",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG = {\n",
    "  \"vocab_size\": 25500, # Vocabulary size\n",
    "  \"context_length\": 64, # Context length\n",
    "  \"emb_dim\": 55, # Embedding dimension\n",
    "  \"n_heads\": 5, # Number of attention heads\n",
    "  \"n_layers\": 12, # Number of layers\n",
    "  \"drop_rate\": 0.1, # Dropout rate\n",
    "  \"qkv_bias\": False # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "154ad776-5b2f-476b-8d85-332763a7d292",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    " def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "  super().__init__()\n",
    "  assert (d_out % num_heads == 0), \"d_out must be divisible by num_heads\"\n",
    "  self.d_out = d_out\n",
    "  self.num_heads = num_heads\n",
    "  self.head_dim = d_out // num_heads\n",
    "  self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "  self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "  self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "  self.out_proj = nn.Linear(d_out, d_out)\n",
    "  self.dropout = nn.Dropout(dropout)\n",
    "  self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    " def forward(self, x):\n",
    "  b, num_tokens, d_in = x.shape\n",
    "  keys = self.W_key(x)\n",
    "  queries = self.W_query(x)\n",
    "  values = self.W_value(x)\n",
    "  keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "  values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "  queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "  keys = keys.transpose(1, 2)\n",
    "  queries = queries.transpose(1, 2)\n",
    "  values = values.transpose(1, 2)\n",
    "  attn_scores = queries @ keys.transpose(2, 3)\n",
    "  mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "  attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "  attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "  attn_weights = self.dropout(attn_weights)\n",
    "  context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "  context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "  context_vec = self.out_proj(context_vec)\n",
    "  return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0d2c6f0-38e4-4dcd-be5d-90409640d8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "  def forward(self, x):\n",
    "    return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b52377b6-a066-4285-8046-57d831b33ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "  def __init__(self, cfg):\n",
    "    super().__init__()\n",
    "    self.layers = nn.Sequential(nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]), GELU(), nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),)\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f90b4bf5-3c6b-41f8-8e4f-aa35817fad75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "  def __init__(self, emb_dim):\n",
    "    super().__init__()\n",
    "    self.eps = 1e-5\n",
    "    self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "    self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "  def forward(self, x):\n",
    "    mean = x.mean(dim=-1, keepdim=True)\n",
    "    var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "    norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "    return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "988cb1f0-bcfd-4f67-8d99-85d8a9cd6406",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "  def __init__(self, cfg):\n",
    "    super().__init__()\n",
    "    self.att = MultiHeadAttention(d_in=cfg[\"emb_dim\"], d_out=cfg[\"emb_dim\"], context_length=cfg[\"context_length\"], num_heads=cfg[\"n_heads\"], dropout=cfg[\"drop_rate\"], qkv_bias=cfg[\"qkv_bias\"])\n",
    "    self.ff = FeedForward(cfg)\n",
    "    self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "    self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "    self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "  def forward(self, x):\n",
    "    shortcut = x\n",
    "    x = self.norm1(x)\n",
    "    x = self.att(x)\n",
    "    x = self.drop_shortcut(x)\n",
    "    x = x + shortcut\n",
    "    shortcut = x\n",
    "    x = self.norm2(x)\n",
    "    x = self.ff(x)\n",
    "    x = self.drop_shortcut(x)\n",
    "    x = x + shortcut\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "059fe16b-4450-4898-974a-2f1bd440aeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_unique_embeddings = unique_embeddings.values\n",
    "emb_tuples = [tuple(row) for row in c_unique_embeddings]\n",
    "emb2idx = {emb: idx for idx, emb in enumerate(emb_tuples)}\n",
    "def get_embedding_ids(batch):\n",
    "    batch_np = batch.detach().cpu().numpy().astype(np.float32)\n",
    "\n",
    "    B, L, D = batch_np.shape\n",
    "    ids = np.full((B, L), -1, dtype=int)\n",
    "\n",
    "    for i in range(B):\n",
    "        for j in range(L):\n",
    "            ids[i, j] = emb2idx.get(tuple(batch_np[i, j]), -1)\n",
    "\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "737c5fea-fc9e-448c-bd5d-c2820305e69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "  def __init__(self, cfg):\n",
    "    super().__init__()\n",
    "    # self.cfg = cfg\n",
    "    self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "    self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "    self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    self.trf_blocks = nn.Sequential(*[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "    self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "    self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "  def forward(self, in_idx):\n",
    "    global device\n",
    "    batch_size, seq_len, _ = in_idx.shape\n",
    "    in_idx = get_embedding_ids(in_idx)\n",
    "\n",
    "    in_idx = torch.from_numpy(in_idx).long().to(device)\n",
    "\n",
    "    tok_embeds = self.tok_emb(in_idx)\n",
    "    pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "    x = tok_embeds + pos_embeds\n",
    "    x = self.drop_emb(x)\n",
    "    x = self.trf_blocks(x)\n",
    "    x = self.final_norm(x)\n",
    "    logits = self.out_head(x)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "71ee7b07-68cf-47ba-87bd-0b30bf724bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(X))\n",
    "train_data = X[:split_idx]\n",
    "val_data = X[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c0d41f87-e61e-4c5f-bb2e-2f3aee072618",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDatasetV1(Dataset):\n",
    "  def __init__(self, txt, max_length, stride):\n",
    "    self.input_ids = []\n",
    "    self.target_ids = []\n",
    "\n",
    "    # for txt in txts:\n",
    "    for i in range(0, len(txt) - max_length, stride):\n",
    "      input_chunk = txt[i:i + max_length]\n",
    "      target_chunk = txt[i + 1: i + max_length + 1]\n",
    "      self.input_ids.append(torch.tensor(input_chunk))\n",
    "      self.target_ids.append(torch.tensor(target_chunk))\n",
    "  def __len__(self):\n",
    "    return len(self.input_ids)\n",
    "  def __getitem__(self, idx):\n",
    "    return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c1715d11-c246-4b2d-9182-989bc9baf57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "  dataset = GPTDatasetV1(txt, max_length, stride)\n",
    "  dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "  return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "62c9d170-b748-46d1-92ab-6c57249c4aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = create_dataloader_v1(\n",
    "  train_data,\n",
    "  batch_size=256,\n",
    "  max_length=GPT_CONFIG[\"context_length\"],\n",
    "  stride=GPT_CONFIG[\"context_length\"],\n",
    "  drop_last=True,\n",
    "  shuffle=True,\n",
    "  num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "  val_data,\n",
    "  batch_size=256,\n",
    "  max_length=GPT_CONFIG[\"context_length\"],\n",
    "  stride=GPT_CONFIG[\"context_length\"],\n",
    "  drop_last=False,\n",
    "  shuffle=False,\n",
    "  num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6e6fe794-967e-4d21-8c6c-789adbe1dc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_emb_batch, model, device):\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_emb_batch = target_emb_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    target_ids = get_embedding_ids(target_emb_batch)\n",
    "    target_ids = torch.from_numpy(target_ids).long().to(device)\n",
    "    B, L, V = logits.size()\n",
    "    logits_flat  = logits.view(B * L, V)\n",
    "    targets_flat = target_ids.view(B * L)\n",
    "    loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "23343994-61f4-4981-9474-a198c04c65f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "  total_loss = 0\n",
    "  if len(data_loader) == 0:\n",
    "    return float(\"nan\")\n",
    "  elif num_batches is None:\n",
    "    num_batches = len(data_loader)\n",
    "  else:\n",
    "    num_batches = min(num_batches, len(data_loader))\n",
    "\n",
    "  for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "    if i < num_batches:\n",
    "      loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "      total_loss += loss.item()\n",
    "    else:\n",
    "      break\n",
    "\n",
    "  return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e0334e14-7dc4-4bb5-83f7-6c5713568e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "  model.train()\n",
    "  return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b24bc405-5da0-47d3-8dc6-6149871cf77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter):\n",
    "  train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "  tokens_seen, global_step = 0, -1\n",
    "  for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for input_batch, target_batch in train_loader:\n",
    "      optimizer.zero_grad()\n",
    "      loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      tokens_seen += input_batch.numel()\n",
    "      global_step += 1\n",
    "\n",
    "      if global_step % eval_freq == 0:\n",
    "        train_loss, val_loss = evaluate_model(model, train_loader, val_loader, device, eval_iter)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        track_tokens_seen.append(tokens_seen)\n",
    "        print(f\"Ep {epoch+1} (Step {global_step:06d}): Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "  return train_losses, val_losses, track_tokens_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d42c602a-aa45-4a6b-b8c5-09c08767281d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 10.279, Val loss 10.181\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "model = GPTModel(GPT_CONFIG)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(\n",
    "  model.parameters(),\n",
    "  lr=0.0004, weight_decay=0.1\n",
    ")\n",
    "num_epochs = 1\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "  model, train_loader, val_loader, optimizer, device,\n",
    "  num_epochs=num_epochs, eval_freq=5, eval_iter=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bc41cf6e-9a97-454b-b281-42f6dabcb418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.247915272635977\n",
      "7.98386254184753\n",
      "8.656336474587334\n",
      "7.98886252763068\n",
      "9.023997517736388\n",
      "7.9750229206168655\n",
      "16.514930167676898\n",
      "18.385314779849054\n",
      "14.545465816013378\n",
      "9.38739325036704\n",
      "18.368198044579138\n",
      "7.974583962582878\n",
      "16.482078549387225\n",
      "18.349080441232072\n",
      "16.469640194422478\n",
      "7.910358993124892\n",
      "7.901534467248158\n",
      "11.247022521741265\n",
      "7.731475566131577\n",
      "7.308727171859933\n",
      "9.051127858063138\n",
      "7.181341335591885\n",
      "6.6530181219617575\n",
      "14.447353068641473\n",
      "6.584729090870785\n",
      "6.042838385452925\n",
      "16.295238730671368\n",
      "6.036860324389807\n",
      "7.432046688129004\n",
      "14.341121710156694\n",
      "9.995226306048066\n",
      "12.294837781310628\n",
      "6.192305225701958\n",
      "14.464475654862516\n",
      "11.570015464702\n",
      "15.180641138247406\n",
      "9.218631254207711\n",
      "11.067600412404124\n",
      "11.751574571617299\n",
      "14.254624330144091\n",
      "8.00738048280576\n",
      "17.322060050525973\n",
      "17.062386470696577\n",
      "6.079094311430087\n",
      "6.38252465711247\n",
      "14.68425207520717\n",
      "10.860253993230389\n",
      "15.429088228469729\n",
      "14.589496794282729\n",
      "14.548395814370437\n",
      "16.5712313391209\n",
      "15.469748308063071\n",
      "7.017635114676915\n",
      "6.624335811615548\n",
      "6.338946814994523\n",
      "17.024612832446802\n",
      "6.61619457992772\n",
      "8.906899679511074\n",
      "7.579009828578151\n",
      "10.198663318519701\n",
      "14.55754635945672\n",
      "16.758573665639744\n",
      "15.612570607096448\n",
      "16.380664876472064\n",
      "8.709897526262386\n",
      "14.088071361281605\n",
      "18.263309605716906\n",
      "8.380923546811578\n",
      "16.42945578410033\n",
      "14.529281438331306\n",
      "8.54256396628695\n",
      "10.588538393540034\n",
      "16.78495514628805\n",
      "15.31421236481343\n",
      "16.702834132852477\n",
      "9.407430326359227\n",
      "10.145487371787286\n",
      "8.750293901724657\n",
      "7.879015597214761\n",
      "18.071031952059204\n",
      "12.592365725404767\n",
      "18.08703065198219\n",
      "8.017575177602811\n",
      "13.85290012596176\n",
      "9.19819291238751\n",
      "13.478855880055988\n",
      "15.727114366543116\n",
      "11.84232066722017\n",
      "8.950967591459062\n",
      "10.813387285430329\n",
      "9.097272252007969\n",
      "15.667615433057593\n",
      "16.01132942995427\n",
      "14.62263358641647\n",
      "15.260585540547783\n",
      "10.253229487582262\n",
      "9.199630952903965\n",
      "9.606084451695333\n",
      "17.359182881932778\n",
      "14.086814703176515\n",
      "13.888930899697074\n",
      "15.492539177623875\n",
      "9.101046081208331\n",
      "9.26366525678505\n",
      "9.079487798120022\n",
      "12.211763494746819\n",
      "9.43325803001881\n",
      "9.403149238279253\n",
      "9.3469419944419\n",
      "9.07548760599391\n",
      "8.954259825333391\n",
      "9.170632137674758\n",
      "14.111404648277361\n",
      "9.447228340896993\n",
      "14.171561100523961\n",
      "11.309220495911912\n",
      "9.592244375093827\n",
      "9.708383185675467\n",
      "9.654161258429827\n",
      "11.480653754313757\n",
      "9.059311624239383\n",
      "8.881153046366652\n",
      "11.629472157846335\n",
      "14.187920832824242\n",
      "13.796886222394347\n",
      "17.678457080145098\n",
      "17.507149364215497\n",
      "8.53730633025221\n",
      "14.204536805181107\n",
      "10.636263599231548\n",
      "8.518809869135033\n",
      "8.62021370654504\n",
      "15.461263624997512\n",
      "17.690902279974715\n",
      "8.758064202480586\n",
      "8.203277322461735\n",
      "13.94341138865664\n",
      "8.989534437223929\n",
      "8.637153192992308\n",
      "8.782820575609719\n",
      "9.151140011445577\n",
      "11.317950266749042\n",
      "8.48779298789674\n",
      "10.967994578199272\n",
      "10.99311407894153\n",
      "8.733787920573127\n",
      "13.972623861415233\n",
      "8.483968660481757\n",
      "8.345532927832341\n",
      "9.801257373426084\n",
      "9.13962726672579\n",
      "11.014147364686911\n",
      "14.226124280068117\n",
      "8.563644763960099\n",
      "8.649251091268162\n",
      "10.18428103770777\n",
      "12.011730650905013\n",
      "12.03666921715157\n",
      "12.0614363846739\n",
      "7.8069574877633405\n",
      "8.410201040524353\n",
      "7.716003664916889\n",
      "8.365584140688377\n",
      "16.48801228605985\n",
      "15.020896121257213\n",
      "8.361007966797029\n",
      "15.472270351083653\n",
      "12.175738158818108\n",
      "12.199438149504381\n",
      "8.26797545825999\n",
      "7.692962337801681\n",
      "8.225219744967477\n",
      "18.962379895131072\n",
      "8.200711851295134\n",
      "17.680219291205375\n",
      "7.188734927590237\n",
      "7.120787322703621\n",
      "10.03972755724075\n",
      "11.100748109170311\n",
      "17.699075563121937\n",
      "19.381372215047495\n",
      "15.175687578790903\n",
      "11.75290379452327\n",
      "9.641474806892411\n",
      "11.312134376270913\n",
      "14.133345807499827\n",
      "9.86102841208765\n",
      "12.33202719600919\n",
      "6.523965352742664\n",
      "10.333438592749616\n",
      "8.182869819911838\n",
      "17.805470766003953\n",
      "15.527855070474272\n",
      "6.709998516321847\n",
      "9.013616375314564\n",
      "10.23566219822781\n",
      "15.414725695113463\n",
      "8.231614831063714\n",
      "9.229627582447733\n",
      "19.182738861286378\n",
      "12.008038720535584\n",
      "10.857216757375742\n",
      "7.647562003514527\n",
      "15.496240574766253\n",
      "13.526763938849033\n",
      "15.12822909911985\n",
      "7.704949079771657\n",
      "7.6447683146969485\n",
      "12.021947068634798\n",
      "10.440271353513673\n",
      "10.487051500447455\n",
      "8.402929091460418\n",
      "11.609499746737491\n",
      "9.984303426726923\n",
      "14.467070556231846\n",
      "15.607077168094975\n",
      "7.487096715049128\n",
      "9.664724071335543\n",
      "7.863011232072863\n",
      "10.525317347302108\n",
      "15.586014318848427\n",
      "12.051800623446722\n",
      "14.537124548202748\n",
      "17.605048189611228\n",
      "6.899536285215858\n",
      "7.14924601007094\n",
      "18.434828649658247\n",
      "17.354642455013824\n",
      "12.0235721476969\n",
      "7.986551101411957\n",
      "12.43316379348938\n",
      "10.637858781476655\n",
      "17.858541664464372\n",
      "7.241371962140698\n",
      "7.1977207225805175\n",
      "6.132764235643052\n",
      "15.493689610424864\n",
      "15.660066096765322\n",
      "11.173992661888262\n",
      "7.987216106802926\n",
      "15.474168114061213\n",
      "12.002574505244056\n",
      "18.097153502869173\n",
      "18.09179203129904\n",
      "11.987521924721031\n",
      "12.02393512925207\n",
      "19.497549047540268\n",
      "7.445549111144377\n",
      "15.388099266400172\n",
      "6.541324361257084\n",
      "9.957683684773086\n",
      "11.68804650851951\n",
      "9.948283075613991\n",
      "11.720553761451109\n",
      "12.642508553991412\n",
      "11.710579788571229\n",
      "12.186189950002111\n",
      "12.666600232132556\n",
      "8.345474033320722\n",
      "8.345474033320722\n",
      "7.471213251736808\n",
      "8.352079396450305\n",
      "19.458141817449558\n",
      "9.085550412287956\n",
      "8.686807772655499\n",
      "19.177814574083474\n",
      "9.233444583116936\n",
      "19.166796120387563\n",
      "16.878843888210334\n",
      "8.633886797051343\n",
      "8.633886797051343\n",
      "18.96374008651867\n",
      "15.314900240568624\n",
      "9.364425914079508\n",
      "14.139587572045988\n",
      "15.33387352564308\n",
      "12.38626544482223\n",
      "9.450192377396883\n",
      "17.34412335828935\n",
      "8.380559332257217\n",
      "17.56739716710471\n",
      "9.392165823497706\n",
      "8.649422510179416\n",
      "18.520614761839933\n",
      "10.066645619974466\n",
      "11.874779945851003\n",
      "12.079842219278603\n",
      "14.997605322923272\n",
      "12.077837208897819\n",
      "18.26789996343705\n",
      "11.633621247321448\n",
      "18.22982586445009\n",
      "14.952786813599761\n",
      "14.86341371323848\n",
      "13.57929767271\n",
      "9.630143112875661\n",
      "11.650096122027291\n",
      "10.133366075198964\n",
      "14.904189872567326\n",
      "9.911049226605272\n",
      "10.133050242465043\n",
      "10.363660211748021\n",
      "10.032942809081266\n",
      "14.74441839941342\n",
      "15.478328543694795\n",
      "10.415949964559188\n",
      "11.02909534943296\n",
      "15.116719882512857\n",
      "12.092215018417072\n",
      "10.331384763194214\n",
      "10.057984318156894\n",
      "9.952286356668411\n",
      "9.923939044532652\n",
      "12.539548762115952\n",
      "9.921412262003482\n",
      "11.806997079209323\n",
      "9.924968695503084\n",
      "13.243580759482207\n",
      "12.523939972869968\n",
      "12.534690750639676\n",
      "12.577473291238162\n",
      "11.750891228669445\n",
      "9.636619738169918\n",
      "9.506485341477076\n",
      "9.78059307863767\n",
      "11.754424361549935\n",
      "12.074850732620094\n",
      "9.197048581285381\n",
      "9.084486342016715\n",
      "18.418116003281998\n",
      "9.828377930721503\n",
      "18.416153026752937\n",
      "10.908002495240517\n",
      "16.567464178135875\n",
      "15.635358978944438\n",
      "8.5978577616173\n",
      "13.476604152589315\n",
      "13.187193647502255\n",
      "10.538878026661056\n",
      "13.439261531977307\n",
      "8.805739506967875\n",
      "8.071681599321007\n",
      "7.966660023184216\n",
      "17.91225618082704\n",
      "10.355844783619718\n",
      "7.353791424681264\n",
      "7.823638155822954\n",
      "17.329171480097855\n",
      "8.693388530896684\n",
      "8.071591131556268\n",
      "8.000820367254818\n",
      "17.957559324848653\n",
      "10.62399912084243\n",
      "19.190053347116653\n",
      "11.380768984589237\n",
      "12.130220015447255\n",
      "7.545951102368945\n",
      "7.499439368848487\n",
      "9.545298248255477\n",
      "14.615348733699344\n",
      "6.296679558767103\n",
      "6.299970886330962\n",
      "15.678586021702335\n",
      "6.977831289109181\n",
      "6.958752230619116\n",
      "11.14896483217718\n",
      "5.576518289699947\n",
      "8.813851688425357\n",
      "16.56385651055489\n",
      "7.145551473910514\n",
      "10.511204845619782\n",
      "18.346833770438288\n",
      "9.29376716534859\n",
      "7.429080419524757\n",
      "5.689865113569281\n",
      "8.5848235578643\n",
      "5.609032441639508\n",
      "11.074273889893307\n",
      "15.67404435343983\n",
      "19.597470484958755\n",
      "7.570216633142249\n",
      "12.017683336360712\n",
      "17.008454647929717\n",
      "19.310240177334\n",
      "15.05594078634531\n",
      "17.59149934842708\n",
      "11.524953288712002\n",
      "7.661438084363318\n",
      "14.88479396273606\n",
      "11.232001756495343\n",
      "17.52527644948446\n",
      "11.254051152705356\n",
      "18.977744045450958\n",
      "11.222487216759703\n",
      "8.14414811659286\n",
      "14.768890931881572\n",
      "16.263869746033805\n",
      "16.56001205243658\n",
      "11.421867013834403\n",
      "15.459275534237003\n",
      "11.339487272743833\n",
      "12.031855764269201\n",
      "18.626802749716816\n",
      "9.634118057920476\n",
      "18.521376799986353\n",
      "9.13416282515221\n",
      "9.204316610132265\n",
      "14.809609490843656\n",
      "16.125224136301227\n",
      "10.105866226484057\n",
      "11.922101386967732\n",
      "9.299348069971701\n",
      "8.98731706416936\n",
      "13.061857718739665\n",
      "10.593007579775513\n",
      "11.737001838888059\n",
      "12.74188429429511\n",
      "11.385057760235846\n",
      "14.06838967438943\n",
      "10.125826717565737\n",
      "11.634414712970212\n",
      "10.849957688298574\n",
      "14.550162700825341\n",
      "10.660541388720176\n",
      "10.657468994226827\n",
      "11.076558381023597\n",
      "11.470649606312428\n",
      "16.132744545302625\n",
      "14.5151469835748\n",
      "12.462847620482075\n",
      "14.849281147597905\n",
      "11.247183902285656\n",
      "11.981574494368123\n",
      "14.95663896513139\n",
      "11.031290015401016\n",
      "11.706111703830176\n",
      "11.31732869147716\n",
      "11.644812013276933\n",
      "12.392477207813643\n",
      "13.26240244542673\n",
      "14.468277111008101\n",
      "12.856304678692519\n",
      "11.993694070072305\n",
      "12.571060022690316\n",
      "15.358590557136512\n",
      "13.803287116582844\n",
      "12.936948473352954\n",
      "12.708202743688599\n",
      "11.392302541888483\n",
      "11.612745759345456\n",
      "10.881410338673334\n",
      "13.254218825830865\n",
      "13.046375692817634\n",
      "15.491244348262445\n",
      "12.270588021949424\n",
      "10.302746187408905\n",
      "10.329529363468582\n",
      "10.720227218622782\n",
      "10.019901118805462\n",
      "12.963163912315537\n",
      "12.388607033796863\n",
      "9.61737736005905\n",
      "9.427588175259466\n",
      "9.324706714348528\n",
      "9.83360484328243\n",
      "14.52902881821161\n",
      "14.256169890945873\n",
      "8.736457846113543\n",
      "14.61356246706212\n",
      "14.335403179785674\n",
      "9.696385131326977\n",
      "12.203018682742911\n",
      "16.66265840646367\n",
      "8.106373805488957\n",
      "7.512397987033596\n",
      "18.753060599385343\n",
      "13.771068868471266\n",
      "14.871474841289723\n",
      "14.837815336332813\n",
      "7.0710712366502\n",
      "7.158587535689034\n",
      "15.178850603346543\n",
      "14.871474841289723\n",
      "14.871474841289723\n",
      "15.145874211768488\n",
      "7.0710712366502\n",
      "14.871474841289723\n",
      "15.178850603346543\n",
      "14.871474841289723\n",
      "15.145874211768488\n",
      "17.98255523063781\n",
      "14.837815336332813\n",
      "8.548189056384114\n",
      "14.804079301162677\n",
      "7.0000034595549865\n",
      "14.837815336332813\n",
      "16.31938698339966\n",
      "14.837815336332813\n",
      "16.349996682383587\n",
      "14.871474841289723\n",
      "15.145874211768488\n",
      "14.837815336332813\n",
      "16.349996682383587\n",
      "16.380549182306257\n",
      "7.730831914559008\n",
      "15.178850603346543\n",
      "7.730831914559008\n",
      "15.178850603346543\n",
      "7.730831914559008\n",
      "16.213687222031574\n",
      "7.158587535689034\n",
      "16.349996682383587\n",
      "7.730831914559008\n",
      "15.178850603346543\n",
      "7.730831914559008\n",
      "13.771068868471266\n",
      "7.730831914559008\n",
      "16.213687222031574\n",
      "7.795239707101001\n",
      "16.213687222031574\n",
      "7.795239707101001\n",
      "13.807329132753317\n",
      "16.244496093565658\n",
      "14.9050583345089\n",
      "13.807329132753317\n",
      "15.802486607343615\n",
      "13.807329132753317\n",
      "15.802486607343615\n",
      "16.244496093565658\n",
      "15.834095584379751\n",
      "13.807329132753317\n",
      "15.802486607343615\n",
      "15.211755508116534\n",
      "7.228096257391873\n",
      "16.244496093565658\n",
      "15.834095584379751\n",
      "15.834095584379751\n",
      "7.228096257391873\n",
      "16.244496093565658\n",
      "15.834095584379751\n",
      "8.779039591190209\n",
      "7.228096257391873\n",
      "16.244496093565658\n",
      "15.834095584379751\n",
      "8.779039591190209\n",
      "15.834095584379751\n",
      "15.244589388983677\n",
      "15.834095584379751\n",
      "16.275246644332917\n",
      "15.865641587256228\n",
      "15.865641587256228\n",
      "13.843494420852652\n",
      "15.802486607343615\n",
      "16.213687222031574\n",
      "15.802486607343615\n",
      "15.211755508116534\n",
      "16.244496093565658\n",
      "15.834095584379751\n",
      "8.779039591190209\n",
      "15.802486607343615\n",
      "16.213687222031574\n",
      "15.802486607343615\n",
      "8.721899801286709\n",
      "15.802486607343615\n",
      "16.213687222031574\n",
      "15.802486607343615\n",
      "13.807329132753317\n",
      "15.802486607343615\n",
      "16.213687222031574\n",
      "15.802486607343615\n",
      "15.211755508116534\n",
      "15.802486607343615\n",
      "15.802486607343615\n",
      "13.807329132753317\n",
      "16.213687222031574\n",
      "16.244496093565658\n",
      "15.834095584379751\n",
      "13.843494420852652\n",
      "15.834095584379751\n",
      "13.843494420852652\n",
      "15.834095584379751\n",
      "7.228096257391873\n",
      "8.721899801286709\n",
      "15.802486607343615\n",
      "13.771068868471266\n",
      "13.771068868471266\n",
      "15.770814277496084\n",
      "16.213687222031574\n",
      "15.802486607343615\n",
      "13.807329132753317\n",
      "15.802486607343615\n",
      "15.770814277496084\n",
      "13.734712875782241\n",
      "7.088397245225637\n",
      "16.18281969663723\n",
      "15.770814277496084\n",
      "13.771068868471266\n",
      "15.770814277496084\n",
      "15.770814277496084\n",
      "15.178850603346543\n",
      "15.739078212375537\n",
      "16.18281969663723\n",
      "8.000203547181506\n",
      "13.771068868471266\n",
      "15.739078212375537\n",
      "16.18281969663723\n",
      "7.158587535689034\n",
      "13.771068868471266\n",
      "15.770814277496084\n",
      "13.771068868471266\n",
      "15.770814277496084\n",
      "15.178850603346543\n",
      "15.770814277496084\n",
      "15.178850603346543\n",
      "15.739078212375537\n",
      "16.18281969663723\n",
      "16.213687222031574\n",
      "16.244496093565658\n",
      "15.834095584379751\n",
      "15.244589388983677\n",
      "15.834095584379751\n",
      "13.843494420852652\n",
      "15.802486607343615\n",
      "16.244496093565658\n",
      "16.275246644332917\n",
      "15.865641587256228\n",
      "15.865641587256228\n",
      "7.365145993537562\n",
      "16.305939204285963\n",
      "15.897124990867825\n",
      "13.91554302857702\n",
      "15.897124990867825\n",
      "16.336574100277875\n",
      "15.928546166404338\n",
      "13.951427804356745\n",
      "10.876517109350237\n",
      "13.91554302857702\n",
      "15.897124990867825\n",
      "16.336574100277875\n",
      "15.897124990867825\n",
      "15.310045905832997\n",
      "15.897124990867825\n",
      "15.310045905832997\n",
      "15.897124990867825\n",
      "10.876517109350237\n",
      "16.336574100277875\n",
      "7.499691694071323\n",
      "15.928546166404338\n",
      "16.336574100277875\n",
      "15.928546166404338\n",
      "15.928546166404338\n",
      "13.951427804356745\n",
      "15.897124990867825\n",
      "13.91554302857702\n",
      "15.897124990867825\n",
      "16.336574100277875\n",
      "16.336574100277875\n",
      "15.928546166404338\n",
      "15.34266944305044\n",
      "15.928546166404338\n",
      "15.34266944305044\n",
      "7.432723290027848\n",
      "13.91554302857702\n",
      "15.897124990867825\n",
      "16.336574100277875\n",
      "15.897124990867825\n",
      "15.897124990867825\n",
      "15.897124990867825\n",
      "16.336574100277875\n",
      "15.928546166404338\n",
      "15.928546166404338\n",
      "15.897124990867825\n",
      "13.91554302857702\n",
      "15.897124990867825\n",
      "7.365145993537562\n",
      "15.865641587256228\n",
      "15.277352703878828\n",
      "15.865641587256228\n",
      "16.305939204285963\n",
      "15.897124990867825\n",
      "15.277352703878828\n",
      "15.865641587256228\n",
      "15.277352703878828\n",
      "15.865641587256228\n",
      "15.865641587256228\n",
      "15.834095584379751\n",
      "16.275246644332917\n",
      "15.865641587256228\n",
      "13.879565475193324\n",
      "15.865641587256228\n",
      "15.834095584379751\n",
      "15.834095584379751\n",
      "15.834095584379751\n",
      "13.843494420852652\n",
      "7.228096257391873\n",
      "13.807329132753317\n",
      "15.802486607343615\n",
      "15.211755508116534\n",
      "15.770814277496084\n",
      "16.213687222031574\n",
      "7.228096257391873\n",
      "15.211755508116534\n",
      "15.770814277496084\n",
      "16.213687222031574\n",
      "15.802486607343615\n",
      "15.802486607343615\n",
      "15.770814277496084\n",
      "13.771068868471266\n",
      "15.770814277496084\n",
      "15.770814277496084\n",
      "13.771068868471266\n",
      "15.770814277496084\n",
      "16.213687222031574\n",
      "15.802486607343615\n",
      "15.211755508116534\n",
      "15.770814277496084\n",
      "16.213687222031574\n",
      "15.802486607343615\n",
      "8.664383194647218\n",
      "15.770814277496084\n",
      "16.213687222031574\n",
      "15.802486607343615\n",
      "15.178850603346543\n",
      "15.770814277496084\n",
      "14.871474841289723\n",
      "16.213687222031574\n",
      "7.228096257391873\n",
      "15.211755508116534\n",
      "15.802486607343615\n",
      "16.213687222031574\n",
      "15.802486607343615\n",
      "13.807329132753317\n",
      "15.802486607343615\n",
      "13.771068868471266\n",
      "7.158587535689034\n",
      "16.213687222031574\n",
      "15.802486607343615\n",
      "13.771068868471266\n",
      "16.213687222031574\n",
      "15.802486607343615\n",
      "7.228096257391873\n",
      "13.807329132753317\n",
      "15.802486607343615\n",
      "15.211755508116534\n",
      "16.213687222031574\n",
      "15.802486607343615\n",
      "13.807329132753317\n",
      "15.802486607343615\n",
      "15.211755508116534\n",
      "16.244496093565658\n",
      "15.802486607343615\n",
      "16.213687222031574\n",
      "15.211755508116534\n",
      "15.802486607343615\n",
      "16.213687222031574\n",
      "15.802486607343615\n",
      "13.807329132753317\n",
      "15.802486607343615\n",
      "16.213687222031574\n",
      "15.802486607343615\n",
      "15.211755508116534\n",
      "15.802486607343615\n",
      "16.213687222031574\n",
      "15.802486607343615\n",
      "13.807329132753317\n",
      "15.802486607343615\n",
      "16.213687222031574\n",
      "15.802486607343615\n",
      "15.802486607343615\n",
      "7.228096257391873\n",
      "16.213687222031574\n",
      "15.802486607343615\n",
      "15.802486607343615\n",
      "15.802486607343615\n",
      "13.771068868471266\n",
      "15.770814277496084\n",
      "15.770814277496084\n",
      "13.771068868471266\n",
      "15.739078212375537\n",
      "16.18281969663723\n",
      "15.770814277496084\n",
      "16.18281969663723\n",
      "16.213687222031574\n",
      "15.802486607343615\n",
      "13.807329132753317\n",
      "15.802486607343615\n",
      "16.213687222031574\n",
      "16.213687222031574\n",
      "15.211755508116534\n",
      "15.802486607343615\n",
      "15.802486607343615\n",
      "16.213687222031574\n",
      "7.158587535689034\n",
      "16.213687222031574\n",
      "15.802486607343615\n",
      "13.807329132753317\n",
      "15.770814277496084\n",
      "16.213687222031574\n",
      "7.228096257391873\n",
      "15.211755508116534\n",
      "15.802486607343615\n",
      "16.244496093565658\n",
      "15.834095584379751\n",
      "15.244589388983677\n",
      "15.834095584379751\n",
      "15.834095584379751\n",
      "13.843494420852652\n",
      "15.802486607343615\n",
      "16.244496093565658\n",
      "15.834095584379751\n",
      "13.843494420852652\n",
      "13.843494420852652\n",
      "13.843494420852652\n",
      "15.834095584379751\n",
      "16.275246644332917\n",
      "15.834095584379751\n",
      "15.244589388983677\n",
      "15.834095584379751\n",
      "16.275246644332917\n",
      "15.865641587256228\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[81]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(forecast_horizon):\n\u001b[32m     16\u001b[39m     logits = model(init_seq)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     next_id = \u001b[43mlogits\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.item()\n\u001b[32m     18\u001b[39m     pred_emb = emb_tuples[next_id]\n\u001b[32m     19\u001b[39m     predicted_embs.append(pred_emb)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "context = GPT_CONFIG[\"context_length\"]\n",
    "forecast_horizon = context\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for t in range(len(val_data) - context - forecast_horizon):\n",
    "        init_seq = val_data[t : t+context]\n",
    "        init_seq = init_seq[np.newaxis]\n",
    "        init_seq = torch.tensor(init_seq, device=device)\n",
    "        true_seq = val_data[t+context : t+context+forecast_horizon]\n",
    "\n",
    "        # print(pred_emb)\n",
    "        predicted_embs = []\n",
    "        for _ in range(forecast_horizon):\n",
    "            logits = model(init_seq)\n",
    "            next_id = logits[0, -1].argmax().item()\n",
    "            pred_emb = emb_tuples[next_id]\n",
    "            predicted_embs.append(pred_emb)\n",
    "        # print(predicted_embs)\n",
    "        # print(true_seq)\n",
    "        dist = np.linalg.norm(predicted_embs - true_seq)\n",
    "        print(dist)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ca679149-979f-4b91-9686-ab9a12f81730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1561, -0.3602,  0.2979,  ...,  0.3252,  0.2796, -1.4058],\n",
      "       device='cuda:0')\n",
      "tensor([-0.3346, -0.2107,  1.0144,  ...,  0.8399,  0.7852, -0.8823],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for m in model.modules():\n",
    "    if isinstance(m, nn.Dropout):\n",
    "        m.train()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for t in range(1):\n",
    "        init_seq = val_data[t : t+context]\n",
    "        init_seq = init_seq[np.newaxis]\n",
    "        init_seq = torch.tensor(init_seq, device=device)\n",
    "        true_seq = val_data[t+context : t+context+forecast_horizon]\n",
    "\n",
    "        # print(pred_emb)\n",
    "        predicted_embs = []\n",
    "        for _ in range(2):\n",
    "            logits = model(init_seq)\n",
    "            print(logits[0, -1])\n",
    "            predicted_embs.append(logits[0, -1])\n",
    "        \n",
    "        predicted_embs = torch.cat(predicted_embs, 0)\n",
    "        # print(predicted_embs)\n",
    "        mean = predicted_embs.mean(0)\n",
    "        # print(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6909b342-4b53-4706-ac0c-9bc9783bd3b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (anomaly)",
   "language": "python",
   "name": "anomaly"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
